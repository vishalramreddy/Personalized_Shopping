# -*- coding: utf-8 -*-
"""Recommendation System for Amazon data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ECkiJT8q7U1Z0XdXwIFm4jcTEq41zOQ1

The present notebook deals with the EDA(Exploratory Data Analysis) of the Amazon sales Dataset.This is going to be the first step towards generating recommendation system

**The first step is to import necessary Libraries**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy as sp

"""**Data Loading and Exploration Cleaning**"""

df=pd.read_csv('amazon.csv') # reading the content in csv file and generating data frames

pd.set_option('display.max_columns',None) # to display all the columns

df.head(5) # to get a look of first five columns of the datatset

df.columns # to get the name of all the columns

print(df.shape) # to get the shape of the dataset

df.info() # to get the information about the dataset

df.isnull().sum() # to get the null values in the dataset

"""**Observation Set 1**

*There are 1465 rows and 16 columns in the dataset.*

*The data type of all columns is object.*

*The columns in the datasets are:*

   'product_id', 'product_name', 'category', 'discounted_price', 'actual_price', 'discount_percentage', 'rating', 'rating_count', 'about_product', 'user_id', 'user_name', 'review_id', 'review_title', 'review_content', 'img_link', 'product_link'

*There are a few missing values in the dataset, which we will read in detail and deal with later on in the notebook.*

**Changing the data types of columns from object to float**
"""

# changing the datatype of dicounted price and actual price

df['discounted_price']=df['discounted_price'].str.replace('₹','')
df['discounted_price']=df['discounted_price'].str.replace(',','')
df['discounted_price']=df['discounted_price'].astype('float')

df['actual_price']=df['actual_price'].str.replace('₹','')
df['actual_price']=df['actual_price'].str.replace(',','')
df['actual_price']=df['actual_price'].astype('float')

# changing Datatype and values in Dicounted Percentage

df['discount_percentage']=df['discount_percentage'].str.replace('%','')
df['discount_percentage']=df['discount_percentage'].astype('float')

# Finding Unusual String
df['rating'].value_counts()

# Check the strange row
df[df['rating']=='|']

"""I got this product rating on Amazon by searching the provided product_id on their official website (amazon.in)

**The rating is 3.9. So, I am going to give the item rating a 3.9 as well.**
"""

# Changing the rating cloumn datatype
df['rating']=df['rating'].str.replace('|','3.9').astype('float')

# Changing the rating_count column datatype
df['rating_count']=df['rating_count'].str.replace(',','').astype('float')

df.info()

df.describe()

"""**Observation Set 2**



* *All Columns data type was object So,I converted the column datatype to float*
*   *there are 4 numeric as per the python descriptive stats from python describe function*

**Dealing With Missing Values**
"""

df.isnull().sum().sort_values(ascending=False)

# Find the Missing Values percentage in the data
missing_values_percentage=round((df.isnull().sum()/df.shape[0])*100,2)
missing_values_percentage.sort_values(ascending=False)

#Find the total missing values
total_missing_values=df.isnull().sum().sum()
total_missing_values

"""**Let's plot the Missing Values**"""

# make a figure size
plt.figure(figsize=(22,10))

# plot the null values in each column
plt.bar(missing_values_percentage.index,missing_values_percentage.values,color='red')

"""**We are only viewing the rows where there are null values in the column**"""

df[df['rating_count'].isnull()]

# Impute the missing values
df['rating_count']=df['rating_count'].fillna(value=df['rating_count'].median())

df.isnull().sum().sort_values(ascending=False)

"""***MileStone 1: We have cleaned the dataset from null vallues***

**Duplicates**

Removing duplicates is one of the most important part of the data wrangling process, we must remove the duplicates in order to get the correct insights from the data.

If you do not remove duplicates from a dataset, it can lead to incorrect insights and analysis.

Duplicates can skew statistical measures such as mean, median, and standard deviation, and can also lead to over-representation of certain data points.

It is important to remove duplicates to ensure the accuracy and reliability of your data analysis.
"""

df.duplicated(df.columns).sum()

"""***Milestone 2: So There are no duplicates dataset***

**Data Visualization**
"""

# scatter plot

sns.scatterplot(x='discounted_price',y='actual_price',data=df)
plt.xlabel('Discounted Price')
plt.ylabel('Actual Price')
plt.title('Discounted Price vs Actual Price')
plt.show()

# Histogram

sns.histplot(df['rating'],kde=True)
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.title('Distribution of Ratings')
plt.show()

from sklearn.preprocessing import LabelEncoder

le_product_id = LabelEncoder()
le_category = LabelEncoder()
le_review_id = LabelEncoder()
le_review_content = LabelEncoder()
le_product_name = LabelEncoder()
le_user_name = LabelEncoder()
le_about_product = LabelEncoder()
le_user_id = LabelEncoder()
le_review_title = LabelEncoder()
le_img_link = LabelEncoder()
le_product_link = LabelEncoder()

df['product_id'] = le_product_id.fit_transform(df['product_id'])
df['category'] = le_category.fit_transform(df['category'])
#df['review_id'] = le_review_id.fit_transform(df['review_id'])
#df['review_content'] = le_review_content.fit_transform(df['review_content'])
#df['product_name'] = le_product_name.fit_transform(df['product_name'])
df['user_name'] = le_user_name.fit_transform(df['user_name'])
#df['about_product'] = le_about_product.fit_transform(df['about_product'])
df['user_id'] = le_user_id.fit_transform(df['user_id'])
#df['review_title'] = le_review_title.fit_transform(df['review_title'])
df['img_link'] = le_img_link.fit_transform(df['img_link'])
df['product_link'] = le_product_link.fit_transform(df['product_link'])

print(df.head())

"""**Correlation Analysis**"""

correlation_matrix=df.corr()

# print correlation matrix
print(correlation_matrix)

# calculate Spearman correlation coefficients (for non-linear relationships)
spearman_corr = df.corr(method='spearman')

# print Spearman correlation coefficients
print(spearman_corr)

# Text Preprocessing
# Goal: Clean the text to make it suitable for analysis

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
# convert review_content to string
df['review_content'] = df['review_content'].astype(str)

# Download necessary NLTK data
nltk.download('stopwords')
nltk.download('wordnet')

#Initialize lemmatizer and stopwords
lemmatizer=WordNetLemmatizer()
stop_words=set(stopwords.words('english'))

# Text preprocessing function
def preprocessed_text(text):
  # convert to lower case
  text=text.lower()


  # Remove punctuation and numbers
  text=re.sub(r'[^a-z\s]','',text)


  # Tokenize and remove stopwords
  words=text.split()

  words=[lemmatizer.lemmatize(word) for word in words if word not in stop_words]

  # Join the words back into a string
  return ' '.join(words)

# Apply preprocessing
df['cleaned_review']=df['review_content'].apply(preprocessed_text)

print(df.head())

# Sentiment Analysis
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# Download VADER lexicon
nltk.download('vader_lexicon')

# Initialize the sentiment analyzer
sia = SentimentIntensityAnalyzer()

# Calculate Sentiment Scores

# Apply VADER sentiment analysis to calculate scores
df['sentiment_scores'] = df['cleaned_review'].apply(lambda x: sia.polarity_scores(x)['compound'])

# Classify Sentiments
def classify_sentiment(score):
  if score >= 0.05:
    return 'Positive'
  elif score <= -0.05:
    return 'Negative'
  else:
    return 'Neutral'

df['sentiment']=df['sentiment_scores'].apply(classify_sentiment)

# Inspect the results
print(df[['cleaned_review','sentiment_scores','sentiment']].head())

# Visualize Sentiment Distribution
plt.figure(figsize=(8,6))
sns.countplot(x='sentiment',data=df,palette='viridis')
plt.title('Sentiment Distribution')
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.show()

# Visualize average sentiment score per product
if 'product_name' in df.columns:
  avg_sentiment_per_product = df.groupby('product_name')['sentiment_scores'].mean().sort_values()

  # plot average sentiment scores
  plt.figure(figsize=(10,6))
  avg_sentiment_per_product.plot(kind='bar',color='skyblue')
  plt.title('Average Sentiment Score per Product')
  plt.xlabel('Product')
  plt.ylabel('Average Sentiment Score')
  plt.xticks(rotation=90)
  plt.show()
else:
  print("Product name column not found")

#Keyword Extraction steps
from collections import Counter

# Tokenize all the cleaned reviews
all_words=' '.join(df['cleaned_review']).split()

# Count word frequencies
word_freq=Counter(all_words)

# Get the top 20 keywords
top_kewords=word_freq.most_common(20)

# Display the Top keywords
print(top_kewords)
for word,freq in top_kewords:
  print(f'{word}:{freq}')

#Visualize keywords
keywords,frequencies = zip(*top_kewords)

# plot bar chart
plt.figure(figsize=(10,6))
sns.barplot(x=list(frequencies),y=list(keywords),palette='viridis')
plt.title('Top Keywords in reviews')
plt.xlabel('Frequency')
plt.ylabel('Keywords')
plt.show()

# Use of TF-UDF for Important Keywords

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf=TfidfVectorizer(max_features=20)
tfidf_matrix=tfidf.fit_transform(df['cleaned_review'])

# Get feature names (keywords)
tfidf_keywords=tfidf.get_feature_names_out()
tfidf_scores=tfidf.idf_

# Display the top keywords
print('Top Keywords:')
for keyword,score in zip(tfidf_keywords,tfidf_scores):
  print(f"{keyword}:{score}")

# Product Recomendations
# Aggregate Sentiments by Product

if 'product_name' in df.columns:
  product_summary=df.groupby('product_name').agg(avg_sentiment=('sentiment_scores','mean'),review_count=('sentiment_scores','count')).reset_index()
  print(product_summary.head())
else:
  print("Product name column not found")

# Filter Top Products

min_reviews=0
top_products=product_summary[(product_summary['review_count']>min_reviews) & (product_summary['avg_sentiment']>0.2)].sort_values(by='avg_sentiment',ascending=False)
print(top_products)

# Recommend Products

def recommend_products(top_productsn,n=5):
  print(f"Top {n} Recommended Products:")
  for index,row in top_products.head(n).iterrows():
    print(f"Product: {row['product_name']}")
    print(f"Average Sentiment Score: {row['avg_sentiment']}")
    print(f"Number of Reviews: {row['review_count']}")
    print("- Reason: Highly positive customer with sufficient reviews")
    print()

recommend_products(top_products,n=5)

# Visualize top product
plt.figure(figsize=(10,6))
sns.barplot(x='avg_sentiment',y='product_name',data=top_products.head(10),palette='viridis')
plt.title('Top 10 Products by Average Sentiment Score')
plt.xlabel('Average Sentiment Score')
plt.ylabel('Product Name')
plt.show()

# Preview unique product names
unique_products = df['product_name'].unique()
print("Unique Products:", unique_products[:10])  # Display the first 10 unique product names

# Word frequency analysis
from collections import Counter
import re

# Combine all product names into a single text
all_product_names = " ".join(df['product_name'].dropna().astype(str))
all_words = re.findall(r'\w+', all_product_names.lower())  # Tokenize words
word_freq = Counter(all_words)

# Display the most common words
print("Most Common Words in Product Names:", word_freq.most_common(20))